#!/usr/bin/env python3
"""
Script de test et diagnostic GPU pour l'entra√Ænement de mod√®les GAN
V√©rifie la configuration PyTorch + CUDA et les performances
"""

import sys
import time
import logging
import platform
from pathlib import Path

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def test_python_environment():
    """Teste l'environnement Python"""
    logger.info("=== TEST ENVIRONNEMENT PYTHON ===")
    
    # Version Python
    version = sys.version_info
    logger.info(f"Version Python: {version.major}.{version.minor}.{version.micro}")
    
    if version.major < 3 or (version.major == 3 and version.minor < 8):
        logger.error("‚ùå Python 3.8+ requis")
        return False
    else:
        logger.info("‚úÖ Version Python compatible")
    
    # Syst√®me d'exploitation
    logger.info(f"Syst√®me: {platform.system()} {platform.release()}")
    logger.info(f"Architecture: {platform.machine()}")
    
    return True

def test_pytorch_installation():
    """Teste l'installation de PyTorch"""
    logger.info("=== TEST INSTALLATION PYTORCH ===")
    
    try:
        import torch
        import torchvision
        import torchaudio
        
        logger.info(f"‚úÖ PyTorch version: {torch.__version__}")
        logger.info(f"‚úÖ Torchvision version: {torchvision.__version__}")
        logger.info(f"‚úÖ Torchaudio version: {torchaudio.__version__}")
        
        return True
        
    except ImportError as e:
        logger.error(f"‚ùå Erreur d'importation PyTorch: {e}")
        logger.error("Ex√©cutez: python install_pytorch_cuda.py")
        return False

def test_cuda_availability():
    """Teste la disponibilit√© CUDA"""
    logger.info("=== TEST DISPONIBILIT√â CUDA ===")
    
    try:
        import torch
        
        if torch.cuda.is_available():
            logger.info("‚úÖ CUDA disponible")
            logger.info(f"‚úÖ Version CUDA: {torch.version.cuda}")
            logger.info(f"‚úÖ Version cuDNN: {torch.backends.cudnn.version()}")
            return True
        else:
            logger.warning("‚ö†Ô∏è CUDA non disponible")
            logger.warning("V√©rifiez l'installation des drivers NVIDIA")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Erreur test CUDA: {e}")
        return False

def test_gpu_detection():
    """Teste la d√©tection du GPU"""
    logger.info("=== TEST D√âTECTION GPU ===")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            logger.warning("‚ö†Ô∏è Aucun GPU CUDA d√©tect√©")
            return False
        
        # Informations GPU
        gpu_count = torch.cuda.device_count()
        logger.info(f"‚úÖ Nombre de GPU: {gpu_count}")
        
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            logger.info(f"GPU {i}: {props.name}")
            logger.info(f"  VRAM: {props.total_memory / 1e9:.1f} GB")
            logger.info(f"  Compute Capability: {props.major}.{props.minor}")
            logger.info(f"  Multiprocessors: {props.multi_processor_count}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur d√©tection GPU: {e}")
        return False

def test_gpu_memory():
    """Teste la m√©moire GPU"""
    logger.info("=== TEST M√âMOIRE GPU ===")
    
    try:
        import torch
        
        if not torch.cuda.is_available():
            logger.warning("‚ö†Ô∏è GPU non disponible pour test m√©moire")
            return False
        
        device = torch.device('cuda:0')
        
        # M√©moire avant allocation
        torch.cuda.empty_cache()
        memory_before = torch.cuda.memory_allocated(device)
        memory_total = torch.cuda.get_device_properties(device).total_memory
        
        logger.info(f"M√©moire totale: {memory_total / 1e9:.1f} GB")
        logger.info(f"M√©moire utilis√©e avant: {memory_before / 1e6:.1f} MB")
        
        # Test allocation m√©moire
        test_tensor = torch.randn(1000, 1000, device=device)
        memory_after = torch.cuda.memory_allocated(device)
        
        logger.info(f"M√©moire utilis√©e apr√®s: {memory_after / 1e6:.1f} MB")
        logger.info(f"‚úÖ Test allocation m√©moire r√©ussi")
        
        # Nettoyage
        del test_tensor
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur test m√©moire: {e}")
        return False

def test_gpu_performance():
    """Teste les performances GPU"""
    logger.info("=== TEST PERFORMANCES GPU ===")
    
    try:
        import torch
        import torch.nn as nn
        
        if not torch.cuda.is_available():
            logger.warning("‚ö†Ô∏è GPU non disponible pour test performance")
            return False
        
        device = torch.device('cuda:0')
        
        # Test calcul matriciel
        logger.info("Test calcul matriciel...")
        
        # CPU
        start_time = time.time()
        a_cpu = torch.randn(1000, 1000)
        b_cpu = torch.randn(1000, 1000)
        c_cpu = torch.matmul(a_cpu, b_cpu)
        cpu_time = time.time() - start_time
        
        # GPU
        start_time = time.time()
        a_gpu = torch.randn(1000, 1000, device=device)
        b_gpu = torch.randn(1000, 1000, device=device)
        torch.cuda.synchronize()  # Attendre la fin des op√©rations
        
        start_time = time.time()
        c_gpu = torch.matmul(a_gpu, b_gpu)
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0
        
        logger.info(f"Temps CPU: {cpu_time:.4f}s")
        logger.info(f"Temps GPU: {gpu_time:.4f}s")
        logger.info(f"Acc√©l√©ration: {speedup:.1f}x")
        
        if speedup > 5:
            logger.info("‚úÖ Excellentes performances GPU")
        elif speedup > 2:
            logger.info("‚úÖ Bonnes performances GPU")
        else:
            logger.warning("‚ö†Ô∏è Performances GPU limit√©es")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur test performance: {e}")
        return False

def test_mixed_precision():
    """Teste le support Mixed Precision"""
    logger.info("=== TEST MIXED PRECISION ===")
    
    try:
        import torch
        from torch.cuda.amp import autocast, GradScaler
        
        if not torch.cuda.is_available():
            logger.warning("‚ö†Ô∏è GPU non disponible pour test Mixed Precision")
            return False
        
        device = torch.device('cuda:0')
        
        # Test autocast
        with autocast():
            x = torch.randn(100, 100, device=device)
            y = torch.randn(100, 100, device=device)
            z = torch.matmul(x, y)
        
        # Test GradScaler
        scaler = GradScaler()
        
        logger.info("‚úÖ Mixed Precision support√©")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur test Mixed Precision: {e}")
        return False

def test_gan_compatibility():
    """Teste la compatibilit√© pour l'entra√Ænement GAN"""
    logger.info("=== TEST COMPATIBILIT√â GAN ===")
    
    try:
        import torch
        import torch.nn as nn
        
        if not torch.cuda.is_available():
            logger.warning("‚ö†Ô∏è GPU non disponible pour test GAN")
            return False
        
        device = torch.device('cuda:0')
        
        # Test cr√©ation mod√®le simple
        class SimpleGenerator(nn.Module):
            def __init__(self):
                super().__init__()
                self.net = nn.Sequential(
                    nn.Linear(100, 256),
                    nn.ReLU(),
                    nn.Linear(256, 512),
                    nn.ReLU(),
                    nn.Linear(512, 3*64*64),
                    nn.Tanh()
                )
            
            def forward(self, x):
                return self.net(x).view(-1, 3, 64, 64)
        
        # Test sur GPU
        model = SimpleGenerator().to(device)
        noise = torch.randn(16, 100, device=device)
        
        with torch.no_grad():
            fake_images = model(noise)
        
        logger.info(f"‚úÖ Test mod√®le GAN r√©ussi")
        logger.info(f"Forme sortie: {fake_images.shape}")
        
        # Recommandations batch size
        vram_gb = torch.cuda.get_device_properties(device).total_memory / 1e9
        
        if vram_gb < 8:
            recommended_batch = 12
        elif vram_gb < 12:
            recommended_batch = 16
        elif vram_gb < 16:
            recommended_batch = 20
        else:
            recommended_batch = 24
        
        logger.info(f"Batch size recommand√©: {recommended_batch}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erreur test GAN: {e}")
        return False

def generate_report(results):
    """G√©n√®re un rapport de diagnostic"""
    logger.info("=== RAPPORT DE DIAGNOSTIC ===")
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    logger.info(f"Tests r√©ussis: {passed_tests}/{total_tests}")
    
    if passed_tests == total_tests:
        logger.info("üéâ TOUS LES TESTS R√âUSSIS")
        logger.info("Votre configuration est pr√™te pour l'entra√Ænement GAN !")
        logger.info("")
        logger.info("Commandes recommand√©es:")
        logger.info("  python train.py --model dcgan --epochs 10")
        logger.info("  python train.py --model cyclegan --epochs 50")
    elif passed_tests >= total_tests * 0.8:
        logger.info("‚úÖ Configuration majoritairement fonctionnelle")
        logger.info("Quelques optimisations possibles")
    else:
        logger.warning("‚ö†Ô∏è Configuration n√©cessite des corrections")
        logger.info("Consultez le GPU_SETUP_GUIDE.md")

def main():
    """Fonction principale de test"""
    logger.info("=" * 60)
    logger.info("DIAGNOSTIC GPU POUR ENTRA√éNEMENT GAN")
    logger.info("=" * 60)
    
    # Ex√©cution des tests
    results = {
        'python_env': test_python_environment(),
        'pytorch_install': test_pytorch_installation(),
        'cuda_available': test_cuda_availability(),
        'gpu_detection': test_gpu_detection(),
        'gpu_memory': test_gpu_memory(),
        'gpu_performance': test_gpu_performance(),
        'mixed_precision': test_mixed_precision(),
        'gan_compatibility': test_gan_compatibility()
    }
    
    # G√©n√©ration du rapport
    generate_report(results)
    
    logger.info("=" * 60)
    logger.info("DIAGNOSTIC TERMIN√â")
    logger.info("=" * 60)
    
    # Code de sortie
    if all(results.values()):
        sys.exit(0)
    else:
        sys.exit(1)

if __name__ == '__main__':
    main()